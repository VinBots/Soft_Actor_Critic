{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control -> SAC with entropy maximization\n",
    "\n",
    "---\n",
    "\n",
    "This notebook implements the Soft Actor-Critic Algorithm as documented in the paper [here](https://arxiv.org/pdf/1812.05905.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task parameters\n",
    "\n",
    "state_dim = 33\n",
    "action_dim = 4\n",
    "solve_score = 30.0\n",
    "\n",
    "# Key Hyperparameters for simulation-purpose\n",
    "\n",
    "auto_entropy_tuning = False\n",
    "\n",
    "if auto_entropy_tuning==False:\n",
    "    alpha = 0.0035\n",
    "\n",
    "single_q = False\n",
    "\n",
    "#Hyperparameters\n",
    "\n",
    "layer_size=128\n",
    "weights_init_bound = 0.999\n",
    "\n",
    "replay_buffer_size=50000\n",
    "\n",
    "num_epochs=1000\n",
    "\n",
    "num_steps_per_epoch=1000\n",
    "\n",
    "batch_size=256\n",
    "\n",
    "discount=0.99\n",
    "\n",
    "soft_target_tau=0.02\n",
    "\n",
    "target_update_period=1\n",
    "\n",
    "policy_lr=0.0005\n",
    "qf_lr=policy_lr\n",
    "a_lr = policy_lr\n",
    "\n",
    "update_every = 1\n",
    "episods_before_learning = 0\n",
    "\n",
    "first_30 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import random\n",
    "import math\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# CPU / GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Launch the environment\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftAgent(nn.Module):\n",
    "    def __init__(self, input_size, h1_size, h2_size, output_size):\n",
    "        super(SoftAgent, self).__init__()\n",
    "        \n",
    "        # state, hidden layer, action sizes\n",
    "        self.input_size = input_size\n",
    "        self.h1_size = h1_size\n",
    "        self.h2_size = h2_size\n",
    "        self.output_size = output_size \n",
    "        \n",
    "        \n",
    "        # define layers\n",
    "        self.fc1 = nn.Linear(self.input_size, self.h1_size)\n",
    "        self.fc2 = nn.Linear(self.h1_size, self.h2_size)\n",
    "        self.fc3 = nn.Linear(self.h2_size, self.output_size)\n",
    "        \n",
    "        #initialize weights\n",
    "        init_w = 3e-3\n",
    "        self.fc3.weight.data.uniform_(-init_w,init_w)\n",
    "        self.fc3.bias.data.uniform_(-init_w,init_w)            \n",
    "        \n",
    "    def forward(self, state,action):\n",
    "        x = torch.cat([state,action],1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianPolicy(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, h1_size, h2_size, output_mean_size, output_std_size):\n",
    "        super(GaussianPolicy, self).__init__()\n",
    "        \n",
    "        # state, hidden layer, action sizes\n",
    "        self.input_size = input_size\n",
    "        self.h1_size = h1_size\n",
    "        self.h2_size = h2_size\n",
    "        self.output_mean_size = output_mean_size\n",
    "        self.output_std_size = output_std_size\n",
    "\n",
    "        # define layers\n",
    "        self.fc1 = nn.Linear(self.input_size, self.h1_size)\n",
    "        self.fc2 = nn.Linear(self.h1_size, self.h2_size)\n",
    "\n",
    "        self.fc3_mean = nn.Linear(self.h2_size, self.output_mean_size)\n",
    "        self.fc3_log_std = nn.Linear(self.h2_size, self.output_std_size)\n",
    "        \n",
    "        #initialize weights\n",
    "        init_w = 3e-3\n",
    "        self.fc3_mean.weight.data.uniform_(-init_w,init_w)\n",
    "        self.fc3_mean.bias.data.uniform_(-init_w,init_w)\n",
    "        self.fc3_log_std.weight.data.uniform_(-init_w,init_w)\n",
    "        self.fc3_log_std.bias.data.uniform_(-init_w,init_w)\n",
    "                        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.fc3_mean(x) #values of the action should be between -1 and 1 so this is not the mean of the action value\n",
    "        log_std = self.fc3_log_std(x)\n",
    "        \n",
    "        log_std_min = -20\n",
    "        log_std_max = 2\n",
    "        log_std = torch.clamp(log_std,log_std_min, log_std_max)\n",
    "               \n",
    "        return mean,log_std\n",
    "    \n",
    "    def sample (self,state,epsilon = 1e-6):\n",
    "        \n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal (mean,std)\n",
    "        z = normal.rsample()\n",
    "        action = torch.tanh(z)\n",
    "        \n",
    "        log_pi = normal.log_prob(z) - torch.log(1 - action.pow(2) + epsilon)\n",
    "        log_pi = log_pi.sum(1,keepdim=True)\n",
    "        \n",
    "        return action, log_pi\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        normal = Normal(mean, std)\n",
    "        z = normal.sample()\n",
    "        action = torch.tanh(z)\n",
    "        action = action.cpu().detach().squeeze(0).numpy()\n",
    "        return self.rescale_action(action)\n",
    "    \n",
    "    def get_action2(self, state):\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        mean, log_std = self.forward(state)\n",
    "        #std = log_std.exp()\n",
    "        \n",
    "        #normal = Normal(mean, std)\n",
    "        #z = normal.sample()\n",
    "        #action = torch.tanh(z)\n",
    "        action = torch.tanh(mean)\n",
    "        action = action.cpu().detach().squeeze(0).numpy()\n",
    "        return self.rescale_action(action)\n",
    "    \n",
    "    def rescale_action(self, action):\n",
    "        action_range=[-1,1]\n",
    "        return action * (action_range[1] - action_range[0]) / 2.0 +\\\n",
    "            (action_range[1] + action_range[0]) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        #dones = dones.view(dones.size(0), -1)\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def buffer_len(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "qf1 = SoftAgent(\n",
    "    input_size = state_dim + action_dim,\n",
    "    h1_size = layer_size,\n",
    "    h2_size = layer_size,\n",
    "    output_size=1\n",
    ").to(device)\n",
    "qf1_optimizer = optim.Adam(qf1.parameters(), lr=qf_lr)\n",
    "\n",
    "qf2 = SoftAgent(\n",
    "    input_size = state_dim + action_dim,\n",
    "    h1_size = layer_size,\n",
    "    h2_size = layer_size,\n",
    "    output_size=1\n",
    ").to(device)\n",
    "qf2_optimizer = optim.Adam(qf2.parameters(), lr=qf_lr)\n",
    "\n",
    "target_qf1 = SoftAgent(\n",
    "    input_size = state_dim + action_dim,\n",
    "    h1_size = layer_size,\n",
    "    h2_size = layer_size,\n",
    "    output_size=1\n",
    ").to(device)\n",
    "\n",
    "target_qf2 = SoftAgent(\n",
    "    input_size = state_dim + action_dim,\n",
    "    h1_size = layer_size,\n",
    "    h2_size = layer_size,\n",
    "    output_size=1\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# copy parameters of qf1 to target_qf1\n",
    "\n",
    "for target_params, params in zip(target_qf1.parameters(), qf1.parameters()):\n",
    "    target_params.data.copy_(params)\n",
    "\n",
    "for target_params, params in zip(target_qf2.parameters(), qf2.parameters()):\n",
    "    target_params.data.copy_(params)\n",
    "\n",
    "\n",
    "policy = GaussianPolicy(\n",
    "    input_size = state_dim,\n",
    "    h1_size = layer_size,\n",
    "    h2_size = layer_size,\n",
    "    output_mean_size = action_dim,\n",
    "    output_std_size = action_dim\n",
    ").to(device)\n",
    "policy_optimizer = optim.Adam(policy.parameters(), lr=policy_lr)\n",
    "\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size, batch_size, 1)\n",
    "\n",
    "if auto_entropy_tuning:    \n",
    "    target_entropy = -torch.prod(torch.Tensor(4,).to(device)).item()\n",
    "    log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "    alpha = log_alpha.exp()\n",
    "    alpha_optim = optim.Adam([log_alpha], lr=a_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    \n",
    "    global alpha\n",
    "    global log_alpha\n",
    "    global update_step\n",
    "    delay_step = 1\n",
    "    global soft_target_tau\n",
    "    \n",
    "    states, actions,rewards, next_states, dones = replay_buffer.sample() #returns torch tensors\n",
    "\n",
    "    # POLICY ITERATION STEP\n",
    "    #Update the Q-function parameters\n",
    "\n",
    "    next_actions, next_log_pis = policy.sample(next_states)\n",
    "\n",
    "    next_qf1 = target_qf1.forward(next_states,next_actions)\n",
    "    next_qf2 = target_qf2.forward(next_states,next_actions)\n",
    "\n",
    "    #next_qf1_target = next_qf1 - next_log_pis\n",
    "    if single_q:\n",
    "        next_q_target = next_qf1 - alpha * next_log_pis\n",
    "    else:\n",
    "        next_q_target = torch.min(next_qf1,next_qf2) - alpha * next_log_pis\n",
    "\n",
    "    #expected_qf1 = rewards + (1 - dones) * discount * next_qf1_target\n",
    "    expected_q = rewards + (1 - dones) * discount * next_q_target\n",
    "\n",
    "    curr_qf1 = qf1.forward(states,actions)\n",
    "    curr_qf2 = qf2.forward(states,actions)\n",
    "\n",
    "    #qf1_loss = F.mse_loss(curr_qf1, expected_qf1.detach())\n",
    "    qf1_loss = F.mse_loss(curr_qf1, expected_q.detach())\n",
    "    qf2_loss = F.mse_loss(curr_qf2, expected_q.detach())\n",
    "\n",
    "    qf1_optimizer.zero_grad()\n",
    "    qf1_loss.backward()\n",
    "    qf1_optimizer.step()\n",
    "\n",
    "    qf2_optimizer.zero_grad()\n",
    "    qf2_loss.backward()\n",
    "    qf2_optimizer.step()\n",
    "\n",
    "    # POLICY IMPROVEMENT STEP\n",
    "    #new_means, new_stds, new_zs, new_log_pis = policy.sample(states)\n",
    "    #new_actions = torch.tanh(new_zs)\n",
    "    new_actions,log_pi = policy.sample(states)\n",
    "    if update_step % delay_step ==0:\n",
    "        \n",
    "        #new_q = qf1.forward(states, new_actions)\n",
    "        #policy_loss = (alpha * new_log_pis - new_q).mean()\n",
    "        #policy_loss = (new_log_pis - new_q).mean()\n",
    "        min_q = torch.min(qf1.forward(states, new_actions),\n",
    "                          qf2.forward(states, new_actions))\n",
    "\n",
    "        #policy_loss = (- min_q).mean()\n",
    "        policy_loss = (alpha * log_pi - min_q).mean()\n",
    "\n",
    "        #Update policy weights\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        #Updatetargetnetworkweights at every iteration\n",
    "\n",
    "        for target_params, params in zip(target_qf1.parameters(), qf1.parameters()):\n",
    "            target_params.data.copy_(soft_target_tau * params + (1 - soft_target_tau) * target_params)\n",
    "\n",
    "        for target_params, params in zip(target_qf2.parameters(), qf2.parameters()):\n",
    "            target_params.data.copy_(soft_target_tau * params + (1 - soft_target_tau) * target_params)\n",
    "    \n",
    "    #Adjust temperature\n",
    "    # entropy temperature\n",
    "    if auto_entropy_tuning:    \n",
    "        alpha_loss = (log_alpha * (-log_pi - target_entropy).detach()).mean()\n",
    "        alpha_optim.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        alpha_optim.step()\n",
    "        alpha = log_alpha.exp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 7\tLast Score: 1.34; average score: 1.20; alpha: 0"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAxyElEQVR4nO3dd3yV9fn/8deVRQaZJIyEEWTPJBBWxV0Uixq0LqxacSAVZ6daa+u3rV/7s3XiQlG0Im4BFWdrVdQcCCFsZCQBEgiEnOyQ/fn9kRObLyYhQu5zn3E9H4/zMDnnzrnfKpwr9319hhhjUEop5b8C7A6glFLKXloIlFLKz2khUEopP6eFQCml/JwWAqWU8nNBdgf4oeLj401ycrLdMZRSyqusW7fusDEmob3XvK4QJCcnk5WVZXcMpZTyKiKyp6PX9NaQUkr5OS0ESinl57QQKKWUn9NCoJRSfk4LgVJK+TktBEop5ee0ECillJ/TQqCU8kvFlXUsX1+ILsWvhUAp5aeeW53L7a/lsPVAhd1RbKeFQCnllzJznQCsyNlvcxL7aSFQSvmdqrpGNheWA7AyZz/Nzf59e8iyQiAioSKyRkQ2iMgWEbmvnWN6iMhrIrJLRBwikmxVHqWUarVuTylNzYZL0/tTVFGLI89pdyRbWXlFUAecaYxJAVKBmSIy9ahjrgNKjTFDgYeBv1mYRymlAHDklhAUIPxu5kjCQwJZkVNodyRbWVYITIsq17fBrsfR118ZwIuur98EzhIRsSqTUkoBOPKcjOsfTa+ePThnTF9WbTpAXWOT3bFsY2mPQEQCRSQHOAR8YoxxHHVIErAPwBjTCJQDvazMpJTyb0fqm9hYUMaUwS0fNRmpiVTUNvKfb4ttTmYfSwuBMabJGJMK9Acmi8jY43kfEZknIlkiklVc7L//s5RSJy57bykNTYYpJ8UBMH1oPL0iQvz69pBbRg0ZY8qAz4CZR71UCAwAEJEgIBooaefnFxlj0o0x6QkJ7W6wo5RSXeLILSFAIH1QLABBgQGcN74fn247REVtg83p7GHlqKEEEYlxfR0GzAC2H3XYSuDnrq8vBv5tdJqfUspCmXlOxiZFExka/N1zGWlJ1Dc28+HmIhuT2cfKK4J+wGcishFYS0uP4D0R+R8RucB1zGKgl4jsAn4J3GlhHqWUn6ttaCJnXxlTBsf9n+fTBsQwqFc4K/10cpllexYbYzYCae08f2+br2uBS6zKoJRSbeXsK6O+sfm7RnErESEjJZGFn+3iUEUtvaNCbUpoD51ZrJTyG45cJyIw6agrAoALUpNoNrByg/9dFWghUEr5DUdeCaP6RhEdFvy914b27snYpCi/XHtIC4FSyi/UNzaTvbf0u2Gj7ZmdmsSmwnJ2F1d1eIwv0kKglPILGwvKqG34fn+grfPGJyLifyuSaiFQSvmF1oXlJrfTH2jVNzqUaSf1YkWOf21Yo4VAKeUXMnNLGNEnkriIkE6Pm52axJ6SGjYUlLspmf20ECilfF5DUzPr9nTeH2g1c1xfQoICWL7ef5ac0EKglPJ5mwvLqalv6rQ/0CoqNJgzR/TmvY37aWxqdkM6+2khUEr5vK70B9qanZbI4ap6vtr9vaXPfJIWAqWUz3PkljAkIYKEyB5dOv70Eb2JDA3ymxVJtRAopXxaU7MhK7+UKSd1fauT0OBAfjK2Hx9tLuJIve9vWKOFQCnl07bur6CyrvF7C80dS0ZqItX1TXy67aBFyTyHFgKllE9z5LXc55/6A64IAKac1Is+UT384vaQFgKllE/LzHWS3CucPj9wRdHAAOGClET+820xpdX1FqXzDFoIlFI+q7nZsDbf2aVho+3JSE2isdmwavOBbk7mWbQQKKV81vaiSsqPNHRpIll7xiRGMSQhghXrfXvtIS0ESimf1dof+CEjhtoSEWanJrEm30lBaU13RvMoWgiUUj7Lkeukf2wYSTFhx/0eGalJALy7wXdvD2khUEr5JGMMa06gP9BqYK9wJgyM8enRQ1oIlFI+aeehKpzV9cfdH2grIzWJ7UWVbC+q6IZknkcLgVLKJzlyXfMHTvCKAGDW+H4EBgjLfbRprIVAKeWTMvOc9IsOZUDc8fcHWsX37MEpw+J5d8N+mpt9b8MaLQRKKZ9jjMGR62TK4DhEpFvec3ZqEoVlR8jaU9ot7+dJtBAopXxO7uFqDlfVHfew0fbMGN2HsOBAlvtg01gLgVLK5zhyW/Yf+KELzXUmokcQM0b3YdWmA9Q3+taGNVoIlFI+x5FXQkJkDwbHR3Tr+85OS6SspoEvdhR36/vazbJCICIDROQzEdkqIltE5LZ2jjldRMpFJMf1uNeqPEop/2BFf6DVKcMSiA0P9rnbQ0EWvncj8CtjTLaIRALrROQTY8zWo4770hhznoU5lFJ+ZK+zhqKK2m7tD7QKDgxg1vh+vLmugKq6Rnr2sPIj1H0suyIwxhwwxmS7vq4EtgFJVp1PKaXgv/2Bqd3YH2hrdmoStQ3NfLylyJL3t4NbegQikgykAY52Xp4mIhtE5AMRGeOOPEop35WZV0KviBCG9u5pyftPHBRL/9gwluf4zuQyywuBiPQE3gJuN8YcPT87GxhkjEkBHgeWd/Ae80QkS0Syiot9q0mjlOpejlwnky3oD7QSadmwZvXOYoor6yw5h7tZWghEJJiWIrDUGPP20a8bYyqMMVWur1cBwSIS385xi4wx6caY9ISEBCsjK6W8WEFpDYVlR7p12Gh7Zqcl0WzgvY2+cVVg5aghARYD24wxD3VwTF/XcYjIZFeeEqsyKaV823fzByxoFLc1vE8ko/pFscJHbg9ZeUVwMnAVcGab4aE/EZH5IjLfdczFwGYR2QA8BlxujPG9hTyUUm7hyCshJjyYEX0iLT/X7NREcvaVkX+42vJzWc2ysU/GmNVApzfpjDELgYVWZVBK+RdHnpNJyXEEBFjTH2jr/JREHvhwOyty9nPbj4dZfj4r6cxipZRPKCqvZU9JjeX9gVaJMWFMTo5jRU4h3n4jQwuBUsontO5PPNXi/kBbs9OSyD1czeZC796wRguBUsonZOY6iQwNYlS/KLed8ydj+xEcKF6/5IQWAqWUT3DklTApOY5AN/QHWkWHB3P6iN68u2E/TV68YY0WAqWU1ztUWUtucbXb+gNtzU5N4lBlHd/s9t6R71oIlFJeb02ee+YPtOesUb3p2SOIFV58e0gLgVLK6zlynUSEBDI20X39gVahwYHMHNuXDzcXUdvQ5PbzdwctBEopr+fIK2FichxBgfZ8pGWkJlJZ18i/tx+y5fwnSguBUsqrOavr2XGwypb+QKsfDYknIbKH194e0kKglPJqa76bP2BfIQgMEM4fn8hn24spr2mwLcfx0kKglPJqmblOQoMDGJcUY2uO2WmJ1Dc188HmA7bmOB5aCJRSXs2R52TioFhCguz9OBuXFM3g+AivnFymhUAp5bXKaxrYXlTBlMHuHzZ6NBEhIzURR56TA+VH7I7zg2ghUEp5rTX5TozB1kZxW7NTkzAG3t3gXfsUaCFQSnktR24JIUEBpAyIsTsKAMnxEaQMiGH5ei0ESinlFo48J2kDYggNDrQ7yncyUhLZeqCCnQcr7Y7SZVoIlFJeqaK2gS37y21ZVqIz56X0I0Dwqm0stRAopbzSuvxSmg1M9ZD+QKvekaGcPDSeFRu8Z8MaLQRKKa+UmVdCcKCQNjDW7ijfMzs1iX3OI2TvLbU7SpdoIVBKeSVHrpOU/jGEhXhOf6DV2WP60CMowGuaxloIlFJep7qukU2F5UyxcVmJzkSGBvPj0X14f9MBGpqa7Y5zTFoIlFJeZ92eUpqajUdMJOvI7NQknNX1rN552O4ox6SFQCnldRx5JQQGCBMHeV5/oNVpwxOIDgv2iiUntBAopbxOZq6TcUnRRPQIsjtKh0KCAvjJuH58vOUg1XWNdsfplBYCpZRXOVLfxMaCMo/tD7Q1OzWRIw1NfLrtoN1ROqWFQCnlVbL3ltLQZJjqwf2BVpOS40iMDmX5es++PWRZIRCRASLymYhsFZEtInJbO8eIiDwmIrtEZKOITLAqj1LKNzhySwgQSE/23P5Aq4AA4fzURL7YeZiSqjq743TIyiuCRuBXxpjRwFRggYiMPuqYc4Fhrsc84CkL8yilfEBmnpMxidFEhgbbHaVLZqcm0dRsWLXJczessawQGGMOGGOyXV9XAtuApKMOywBeMi0ygRgR6WdVJqWUd6ttaCJnX5nHLDvdFaP6RTGiTyTLPXjtIbf0CEQkGUgDHEe9lATsa/N9Ad8vFkopBUDOvjLqG5s9bqG5Y8lIS2TdnlL2OWvsjtIuywuBiPQE3gJuN8ZUHOd7zBORLBHJKi4u7t6ASimv4ch1IgKTk73nigDggpREAFZ46JwCSwuBiATTUgSWGmPebueQQmBAm+/7u577P4wxi4wx6caY9ISEBGvCKqU8niOvhJF9o4gO947+QKv+seFMSo5lec5+j1yR1MpRQwIsBrYZYx7q4LCVwNWu0UNTgXJjjOd2VJRStqlvbCZ7b6lX9QfaykhNYtehKrYeOK4bI5ay8orgZOAq4EwRyXE9fiIi80VkvuuYVUAusAt4FrjJwjxKKS+2saCM2oZmpnrBRLL2zBrXj6AA8cgNayybn22MWQ3IMY4xwAKrMiilfIcjzwnAZC+YSNae2IgQThuewMqc/fxu5kgCAzr9eHQrnVmslPIKmbklDO/Tk7iIELujHLeMtCSKKmpZ4ypqnkILgVLK4zU0NbNuT6lHLzvdFTNG9SEiJNDjRg9pIVBKebzNheXU1Dd5xUJznQkLCeScMX1ZtekAdY1Ndsf5jhYCpZTH+29/wLsLAcAFqYlU1Dby2XbPmROlhUAp5fEcuSWclBBB78hQu6OcsOlD44nvGcLKDZ5ze0gLgVLKozU1G7Lyvb8/0CooMIDzxify6bZDVNQ22B0H+AGFQETCRGSElWGUUupoW/dXUFnX6LXzB9qTkZpIfWMzH24usjsK0MVCICLnAznAh67vU0VkpYW5lFIKaFlWAvCZKwKA1AExDOoV7jGjh7p6RfAnYDJQBmCMyQEGW5JIKaXayMx1MqhXOH2jvb8/0EpEyEhJ5OvdJRyqqLU7TpcLQYMxpvyo5zxv5SSllE9pbjaszXd67fpCnclIS8IYWLnB/iUnuloItojIFUCgiAwTkceBry3MpZRSbC+qpPxIg0/dFmo1JKEn45KiPWLtoa4WgluAMUAd8ApQDtxuUSallALa9Ad8qFHcVkZqIpsKy9ldXGVrjmMWAhEJBN43xvzeGDPJ9bjHGGP/jS2llE9z5DrpHxtG/9hwu6NY4vyURESw/argmIXAGNMENItItBvyKKUUAMYY1uQ7ffK2UKs+UaH8aEgvVuQU2rphTVdvDVUBm0RksYg81vqwMphSVmlq1nEO3mDnoSqc1fU+e1uoVUZKEntKasjZV2Zbhq4WgreBPwBfAOvaPJTyKmU19cx46HNuWrpOC4KHc+S29Aem+vAVAcDMcX0JCQqw9fZQlzamMca8KCIhwHDXU98aYzxjbrRSXWSM4ddvbCS/pJrcw9X0jdrGveePtjuW6kBmnpN+0aEMiAuzO4qlokKDOWtkb97buJ97Zo0iKND9K/90dWbx6cBO4AngSWCHiJxqXSylut/i1Xl8uu0g98wazdyTk3n+qzxe/Drf7liqHcYYHLkt8wdatj/3bRmpSRyuquer3SW2nL+rW1X+AzjbGPMtgIgMB5YBE60KplR3ytlXxt8+3M7Zo/sw9+Rkmg3scx7hvne30D82jLNG9bE7omoj93A1h6vqmHKSb98WanX6iAQiQ4NYsb6Q04YnuP38Xb0GCW4tAgDGmB1AsDWRlOpe5TUNLFiaTe/IUB68OAURITBAeGxOKmMSo7ll2Xo2Fx49cV7ZyZHbsv+AL84obk9ocCA/GduPj7YUcaTe/RvWdLUQZInIcyJyuuvxLJBlZTCluoMxht+8uYGDFbUsvCKN6PD//v4SHhLE4p+nExMWzLVL1rK/7IiNSVVbjrwSEiJ7MDg+wu4obpORlkh1fROfbjvo9nN3tRD8AtgK3Op6bHU9p5RHW/J1Ph9vPcid544kbWDs917vHRXK83MnUVPfxLVL1lLpIevD+zN/6w+0mjK4F32jQm1ZkbSrhSAIeNQYc5Ex5iLgMSDQulhKnbgN+8q4f9U2fjyqN9dN73ix3JF9o3jyZxPYeaiKm19ZT2NTsxtTqqPtddZQVFHrN/2BVoEBwvkp/fjPt8WUVte79dxdLQT/AtqO4QoDPu3+OEp1j/IjDdy8rKUv8PdLUo75m+WpwxP4y+yxfL6jmD+u3GLrLE9/19ofmOon/YG2MlKTaGw2rNp8wK3n7WohCDXGfLcqkutr31z8Q3k9Ywx3vrWRA2W1PDYnjZjwkC793JzJA5l/2hCWOvby7Je5FqdUHcnMK6FXRAhDe/e0O4rbjUmMYmjvnqxY797JZV0tBNUiMqH1GxFJB7SzpjzSS9/s4YPNRfzmnBFMHPT9vkBnfnvOCGaN68f9q7bzwSb3/lamWjhynUz2s/5AKxFhdmoia/KdFJTWuO28XS0EtwNviMiXIvIl8Cpws2WplDpOmwvL+ev72zhzZG9uOOWkH/zzAQHCPy5NYcLAGG5/LYf1e0stSKk6UlBaQ2HZEb8ZNtqeC1KSAPduWNNpIRCRSSLS1xizFhgJvAY00LJ3cd4xfvZ5ETkkIps7eP10ESkXkRzX497j/HdQCoCK2gYWvJJNr54h/OOSFAICju83ytDgQJ69Op0+UaHc8FIW+5zu+83M3303f8DPGsVtDewVzoSBMax049pDx7oieAZobV9PA+6mZZmJUmDRMX52CTDzGMd8aYxJdT3+5xjHKtUhYwx3vbWJgtIjPD4njdiIrvUFOtKrZw+ev2YSDU2GuUvWUl6jw0rdwZFXQkx4MCP6RNodxVaz05LYXlTJ9qIKt5zvWIUg0BjjdH19GbDIGPOWMeYPwNDOftAY8wXg7OwYpbrLy469vL/pAL8+ewTpyd1zW2Fo7548feVE9pRU84ul66hv1GGlVnPkOZmUHHfcV3O+Yta4fgQGCMvd1DQ+ZiEQkdb1iM4C/t3mta6uU9SZaSKyQUQ+EJExHR0kIvNEJEtEsoqLi7vhtMqXbNlfzp/f28rpIxK48dQf3hfozLQhvXjgovF8vbuEu9/ZpMNKLVRUXsuekhq/7g+06tWzB6cMi+fdDftpdsNy6ccqBMuAz0VkBS2jhL4EEJGhtOxbfCKygUHGmBTgcWB5RwcaYxYZY9KNMekJCe5fkEl5rsraBm5+ZT2x4cEn1BfozE8n9ue2s4bx5roCnvhsV7e/v2rRuj/xVD/uD7Q1OzWJwrIjZO2xfsBCp4XAGPNX4Fe03O+fbv7761AALRvaHzdjTEXr3ARjzCogWETiT+Q9lX8xxnD3O5vZU1LNY5en0atnD8vOdfuPh3FhWhJ//3iHLUsA+IPMXCeRoUGM6hdldxSPMGN0H8KCA1nuhj9vXdmzONMY844xprrNczuMMdkncmIR6SuugcIiMtmVxZ7FuJVXWrZmH+9u2M+vzh5h+SgTEeGBn45j8uA4fvPGRtbma/uruznySpiUHEegn/cHWkX0COLsMX1YtemA5f0py7bCEZFlwDfACBEpEJHrRGS+iMx3HXIxsFlENtCydtHlRm/Aqi7aur+CP727hVOGxfOL04a45Zw9ggJZdNVE+seGccNLWeQdrj72D6kuOVRZS25xtfYHjpKRmkhZTQNf7LC2N2pZITDGzDHG9DPGBBtj+htjFhtjnjbGPO16faExZowxJsUYM9UY87VVWZRvqapr5OZXsokJC+bhy1LdOsIkJjyEF+ZOIkCEuS+scfviYL5qTZ7OH2jPKcMSiIsIsfz2kPs3x1TqBBhjuOedTeSXVPPo5WnEW9gX6MigXhE8e/VE9pfXMu+fWdQ2uH8jEV/jyHUSERLI2ETtD7QVHBjArHH9+HTbQarqGi07jxYC5VVez9rH8pz93P7j4UwbYt9vjxMHxfHQpSmszS/lt29udMsQP1/myCthYnKcLRu3e7qM1ERqG5r5aHORZefQ/+rKa2wvquDeFVs4eWgvFpzR6XxGtzhvfCK/nTmClRv28/CnO+yO47Wc1fXsOFil/YEOTBwUS//YMFZYuPaQFgLlFarrGlmwNJuosGAeuSzNY0aW/OK0IVyWPoDH/72L17P22R3HK635bv6AFoL2iAgZqYms3llMcWWdJefQQqA8njGGPyzfTO7hah69LJWESPf3BToiIvzlwrFMHxrP3W9v4utdh+2O5HUyc52EBgcwLinG7igea3ZqEs0G3ttozVWBFgLl8d5YV8Db6wu59cxh/Gio5805DA4M4MkrJ3BSQgQ3vryOnQcr7Y7kVRx5TiYOiiUkSD+OOjKsTyQZqYnEneBiih3R//LKo+04WMm9KzYz7aRe3HrWMLvjdCgqNJjnr5lEj6BA5i5Za9klvK8pr2lge1EFUwbrsNFjefTyNDJSkyx5by0EymPV1Lf0BXr2COLROake0xfoSP/YcBb/PJ3DVXVc/1IWR+p1WOmxrMl3YgzaKLaZFgLlse5dsYVdxVU8clkavSND7Y7TJSkDYnj08jQ2FpRxx2s5Oqz0GBy5JYQEBZAyIMbuKH5NC4HySG+tK+DNdQXccsZQpg/zvL5AZ84Z05ff/2QUH24p4oEPt9sdx6M58pykDYghNDjQ7ih+TQuB8ji7DlVyz/LNTBkcx20/Hm53nONy3fTBXD1tEIu+yOXlzD12x/FIFbUNbNlfrstKeIDu2FxGqW5zpL6Jm5ZmEx4SyGNzPGe+wA8lItx73mgKSo/wx5Vb6B8bxukjetsdy6Osyy+l2cBU7Q/YTq8IlEf508ot7DxUxcOXpdInyjv6Ah0JCgzg8TlpjOgTyYKl2Wzd7579Z71FZl4JwYFC2sBYu6P4PS0EymMsX1/Ia1n7uOn0IZw63Dd2oovoEcTz10wiMjSY615cy8GKWrsjeQxHrpOU/jGEhWh/wG5aCJRH2F1cxd3vbGJychx3eGlfoCN9o0N5/ppJVBxp4Nola6m2cBVJb1Fd18imwnKm6LISHkELgbJdbUMTC5Zm0yMogEfnpPrkCpSjE6NY+LMJbDtQwa3L1tPk58NK1+0ppanZ6EQyD+F7f+OU17nv3a1sL6rkoctS6RcdZnccy5wxojf3ZYzlX9sP8T/vbsGfN+Rz5JUQGCBMHKT9AU+go4aUrVbkFLJszV7mnzaEM/xgVM1VUwext6SaZ7/MY1CvCK6dPtjuSLZw5DoZlxRNRA/9CPIEekWgbJNbXMXdb29i4qBYfnW2b/UFOnPXuaM4Z0wf/vz+Vj7eYt1mI57qSH0TGwrKtD/gQbQQKFvUNjSx4JX1BAe1DLEM9sG+QEcCAoRHLktjfP8Ybns1h40FZXZHcqv1e0tpaDJM1f6Ax/Cfv33Ko/zl/a1sO1DBQ5emkBjju32BjoSFBPLc1enERYRw3YtZFJTW2B3JbTLznAQIpCdrf8BTaCFQbvfexv28nLmXeaeexJkj+9gdxzYJkT1YMncStQ1NXLcki4raBrsjuYUjt4QxidFEhgbbHUW5aCFQbpV/uJo739pE2sAYfnPOCLvj2G5Yn0ievnIiu4urWLA0m4amZrsjWaq2oYn1+8p02WkPo4VAuU1LXyCbwABh4RUT/Kov0JmTh8Zz/0Xj+HLnYf6wfLNPDyvdsK+M+sZmXWjOw+jYLeU296/axpb9FTx7dTpJftgX6Myl6QPYW1LDws92MahXBL84fYjdkSzhyHMiApOT9YrAk2ghUG6xatMBXvpmD9dPH8yM0f7bF+jML2cMZ4+zhr99uJ2BceHMGt/P7kjdzpFXwsi+UUSHa3/Ak1h2bS4iz4vIIRHZ3MHrIiKPicguEdkoIhOsyqLstbekht+9uZGUATH8duZIu+N4rIAA4cGLx5M+KJY7Xs9h3Z5SuyN1q/rGZtbtKdX+gAey8ibtEmBmJ6+fCwxzPeYBT1mYRdmkrrGlLyACC+ekERKkfYHOhAYHsujqdBKjQ7nhpSz2lvjOsNKNBWXUNjQzVSeSeRzL/lYaY74AnJ0ckgG8ZFpkAjEi4nvXwn7uf1dtZ1NhOQ9eksKAuHC743iFuIgQXpg7mWZjuGbJGspq6u2O1C0ceS0fB5N1IpnHsfPXsyRgX5vvC1zPfY+IzBORLBHJKi4udks4deI+3FzEkq/zmXtyMueM6Wt3HK8yOD6CRVelU+A8wryX1uGs9v5ikJlbwvA+PYmLCLE7ijqKV1ynG2MWGWPSjTHpCQm+sWGJr9vnrOE3b25gfP9o7jp3lN1xvNLkwXH8/dIU1u8r5eyHP2fVpgN2RzpuDU2t/QG9GvBEdhaCQmBAm+/7u55TXq6+sZmbX8kGYOGcCdoXOAEXpCSy8ubp9I0O5aal2SxYms3hqjq7Y/1gmwvLqalv0oXmPJSdf0NXAle7Rg9NBcqNMd77K4/6zt8+3M6GgnIevHg8A3tpX+BEjeoXxTs3ncxvzhnBJ1sPcvbDX7Byw36vmnj23/6AFgJPZOXw0WXAN8AIESkQketEZL6IzHcdsgrIBXYBzwI3WZVFuc/HW4pYvDqPn08bxMyx2vvvLsGBASw4Yyjv3TqdAXHh3LpsPTf+cx2HKr1jD2RHbgknJUTQOzLU7iiqHZZNKDPGzDnG6wZYYNX5lfsVlNbw6zc2MDYpirtnaV/ACsP7RPLW/GksXp3HPz7ZwYyHvuBPF4xmdmoSImJ3vHY1NRuy8ks5LyXR7iiqA3rzVnWLhqZmblm2HmPgiSsm0CMo0O5IPisoMIAbTxvCqltPYUhCBHe8toHrX8yiqNwzrw627q+gsq5R5w94MC0Eqls8+NG3rN9bxgM/Hc+gXhF2x/ELQ3v35I35P+KeWaP4avdhZjz8Oa9n7fO43oEjrwRARwx5MC0E6oQtX1/Ioi9yuXLqQJ9cH8eTBQYI159yEh/cdiqj+kbx2zc3cs0La9lfdsTuaN/JzHUyqFc4faO1P+CptBCoE/J61j7ueD2HKYPjuGfWaLvj+K3B8RG8Om8q910whjV5Ts5++AuWrdlr+9VBc7Nhbb5T1xfycFoI1HF7OXMPv31zI9OHxrNk7mRCg7UvYKeAAOHnP0rmo9tPZVxSNHe9vYmrFq9hn9O+9Yq2F1VSfqRBbwt5OC0E6rgsXp3HPcs3c9bI3jx7dTphIVoEPMXAXuEsvX4Kf5k9lvV7S5n5yBf8M3MPzc3uvzr4rj+gjWKPpoVA/WBPfLaLP7+3lXPH9uWpKyfqlYAHCggQrpw6iI/uOJUJg2L5w/LNXPFcpttXM3XkOkmKCaN/rE4s9GRaCFSXGWN46JMdPPjRt2SkJvK4Livt8frHhvPStZN54KJxbCms4JxHvuCFr/LccnVgjGFNvlOvBryA/i1WXWKM4YEPt/PYv3ZyycT+PHRpKkG657BXEBEunzyQj+44lSknxXHfu1u5bNE35B2utvS8Ow9V4ayuZ6r2Bzye/k1Wx2SM4b53t/LM5y1DRP/20/EEBnjmLFbVscSYMF64ZhIPXjyeb4sqmfnIFzz3ZS5NFl0dOHK1P+At/KYQlNXU878fbKO2ocnuKF6ludlw9zubWfJ1PteePJg/Z4wlQIuA1xIRLkkfwCe/PI1ThsXzl/e3ccnTX7PrUFW3nyszz0nfqFAG6oZEHs9vCsHnO4p55vNcLnrya/aUWHtJ7Cuamg2/eXMjy9bs5abTh/CH80Z57Ho26ofpExXKs1en88hlqeQeruYnj33JU//ZTWNTc7e8vzEGR25Lf0D/zHg+vykEGalJvHDNJArLjnDe46v5dOtBuyN5tIamZm57dT1vZRfwyxnD+c05I/QvtI8REWanJfHxHadyxogE/vbhdn761NfsOFh5wu+de7iaw1V1On/AS/hNIQA4Y2Rv3rtlOsm9Irj+pSwe/Gi7ZfdHvVldYxMLlmbz3sYD3HnuSG49a5gWAR/WOzKUp6+cyMIr0thXeoTzHlvNwn/vpOEErg4cuS37D2h/wDv4VSEAGBAXzhvzpzFn8kCe+Gw3Vz/v8Modn6xS29DE/H+u4+OtB/nj+aOZf9oQuyMpNxARzhufyCd3nMqMMX34+8c7uPDJr9h2oOK43s+RV0J8zx6cFK8LEHoDvysEAKHBgfzvReN48OLxLeukP7aadXtK7Y5lu5r6Rq57cS3/2VHM/ReOY+7Jg+2OpNysV88ePHHFBJ762QSKyms5//HVPPLpDuobu351oP0B7+OXhaDVJekDePumHxESFMBlz3zDkq/ybF+kyy5VdY1c8/xavtldwoMXp3DFlIF2R1I2OndcPz654zRmje/HI5/u5IKFq9lcWN6ln93rrKGoopaputCc1/DrQgAwJjGad2+ZzukjEvjTu1u59dUcqusa7Y7lVuVHGrhqsYN1e0t59PI0Lp7Y3+5IygPERoTw6OVpPHt1OiXV9WQ88RV//+hb6ho7H4L93/6ANoq9hd8XAoDosGAWXZXOb2eO4P2N+8l44itLxlV7otLqen72XCabC8t54ooJnK/bCaqjzBjdh0/vOI3ZqUks/GwX5z++mg37yjo8PjOvhLiIEIb17um+kOqEaCFwCQgQbjp9KC9fN4XS6noyFq7m/Y0H7I5lqeLKOuY8m8mOg1UsuiqdmWP72h1Jeajo8GD+cWkKL1wziYojjVz45Fc88MH2didoOnKdTE7W/oA30UJwlB8Njef9W09hRN9IFrySzZ/f23pCw+g8VVF5LZcv+ob8kmqe//kkzhjZ2+5IygucMbI3H//yVC6ZOICnP9/NrMe+JHvvfwdaFJTWUFh2RIeNehktBO3oGx3Kq/Omcc2Pklm8Oo85izI5WOGZG4Mfj4LSGi5b9A1F5bW8OHcy04fF2x1JeZGo0GD+dvF4Xrp2MrUNzVz81Nf89f2t1DY0/bc/oBPJvIp42yiZ9PR0k5WV5bbzrdywnzvf2kh4SCCPz5nAtCHe/Qd8T0k1VzzroKK2gRevncyEgbF2R1JerLK2gQc+2M5Sx14Gx0fQO7IH24sqWf+HGbomlYcRkXXGmPT2XtMrgmO4ICWRFQtOJjosmJ89l8nTn+/22iGmu4uruPSZb6iub2TZDVO1CKgTFhkazF8vHMcr10+hoakZR56TSclxWgS8jF4RdFFVXSO/e2sj7288wNmj+/D3S1OICg12e47j9W1RJT97zoExhqU3TGFk3yi7IykfU13XyJKv85k+NJ6UATF2x1FH6eyKQAvBD2CM4YWv8rl/1Tb6x4bx1JUTGdXP8z9QNxeWc9ViB8GBAbxywxSG9o60O5JSys1suzUkIjNF5FsR2SUid7bz+jUiUiwiOa7H9VbmOVEiwrXTB/PqvKkcaWjiwie/4u3sArtjdWr93lKueDaT8JAgXr9xmhYBpdT3WFYIRCQQeAI4FxgNzBGR0e0c+poxJtX1eM6qPN0pPTmO9245hdQBMfzy9Q38/p1Nx5xtaYe1+U6uWryGmPAQXrtxKsm6AJhSqh1WXhFMBnYZY3KNMfXAq0CGhedzq4TIHrx83RTmnzaEpY69XPL0NxSU1tgd6ztf7zrM1YvX0DuyB6/fOI3+sbpLlFKqfVYWgiRgX5vvC1zPHe2nIrJRRN4UkQHtvZGIzBORLBHJKi4utiLrcQkKDODOc0fyzFUTySuu5rzHV/Ofbw/ZHYv/fHuIuUvWMiAujFdvnErf6FC7IymlPJjdw0ffBZKNMeOBT4AX2zvIGLPIGJNujElPSEhwa8CuOGdMX969ZTp9o0KZu2Qtj3y6g2abNrz5eEsRN7yUxZCEnrw6bxq9I7UIKKU6Z2UhKATa/obf3/Xcd4wxJcaY1l1hngMmWpjHUsnxEbxz08lcmJbEI5/uZO6StZRW17s1w3sb93PT0mxGJ0az7IapxEWEuPX8SinvZGUhWAsME5HBIhICXA6sbHuAiPRr8+0FwDYL81guLCSQf1ySwv0XjuOb3SWcd4xVGrvT29kF3LpsPWkDY3j5uslEh3vPHAellL0sKwTGmEbgZuAjWj7gXzfGbBGR/xGRC1yH3SoiW0RkA3ArcI1VedxFRLhiykDe/MU0AC55+huWOvZYOht52Zq9/OqNDUw9qRcvXjuZSC+a6KaUsp9OKLNQaXU9t7+Ww+c7irloQhJ/nT2OsJDAbj3Hi1/n88eVWzhteALPXDWR0ODufX+llG/QtYZsEhsRwgvXTOKOHw/nnfWFXPjkV+Qfru6291/0xW7+uHILM0b3YdHVWgSUUsdHC4HFAgKE2348jCVzJ1NU0bIZ+Mdbik74fR//107uX7WdWeP68eTPJtAjSIuAUur4aCFwk9OGJ/DeLdM5KSGCef9cxwMfbKfxODa8Mcbw94++5R+f7OCitCQevTyV4ED936iUOn76CeJG/WPDeX3+NK6cOpCnP9/NlYsdFFfWHfsHXYwx/PX9bSz8bBeXTxrA3y9JIUiLgFLqBOmniJv1CArkL7PH8dClKeTsK2PWY1+Sle885s81NxvuXbGF51bn8fNpg7j/wnG65rtSqltoIbDJRRP6885NJxMeEsjlizJZvDqvwyGmTc2Gu97exD8z9zDv1JP40wVjtAgopbqNFgIbjeoXxcpbpnPmyN78+b2t3LxsPVV1jf/nmMamZn71eg6vZe3j1jOHcte5IxHRIqCU6j5aCGwWFRrMM1dN5K5zR/LBpgNkLFzNzoOVADQ0NXPrq+tZnrOfX589nF+ePUKLgFKq22kh8AAiwo2nDWHp9VMpP9JIxhMtG9784uVsVm0q4p5Zo7j5zGF2x1RK+SidWexhDlbUcvMr2azNLwXgzxljuGpasr2hlFJer7OZxUHuDqM61ycqlFdumMozn+9mUK8Izk9JtDuSUsrHaSHwQMGBAXorSCnlNtojUEopP6eFQCml/JwWAqWU8nNaCJRSys9pIVBKKT+nhUAppfycFgKllPJzWgiUUsrPed0SEyJSDOw5zh+PBw53YxyreVNeb8oK3pXXm7KCd+X1pqxwYnkHGWMS2nvB6wrBiRCRrI7W2vBE3pTXm7KCd+X1pqzgXXm9KStYl1dvDSmllJ/TQqCUUn7O3wrBIrsD/EDelNebsoJ35fWmrOBdeb0pK1iU1696BEoppb7P364IlFJKHUULgVJK+Tm/KQQiMlNEvhWRXSJyp915OiMiz4vIIRHZbHeWYxGRASLymYhsFZEtInKb3Zk6IiKhIrJGRDa4st5nd6auEJFAEVkvIu/ZnaUzIpIvIptEJEdEPH4/WRGJEZE3RWS7iGwTkWl2Z2qPiIxw/TdtfVSIyO3deg5/6BGISCCwA5gBFABrgTnGmK22BuuAiJwKVAEvGWPG2p2nMyLSD+hnjMkWkUhgHTDbE//biogAEcaYKhEJBlYDtxljMm2O1ikR+SWQDkQZY86zO09HRCQfSDfGeMUELRF5EfjSGPOciIQA4caYMptjdcr1WVYITDHGHO/E2u/xlyuCycAuY0yuMaYeeBXIsDlTh4wxXwBOu3N0hTHmgDEm2/V1JbANSLI3VftMiyrXt8Guh0f/JiQi/YFZwHN2Z/ElIhINnAosBjDG1Ht6EXA5C9jdnUUA/KcQJAH72nxfgId+WHkzEUkG0gCHzVE65LrNkgMcAj4xxnhsVpdHgN8CzTbn6AoDfCwi60Rknt1hjmEwUAy84Lrt9pyIRNgdqgsuB5Z195v6SyFQFhORnsBbwO3GmAq783TEGNNkjEkF+gOTRcRjb72JyHnAIWPMOruzdNF0Y8wE4FxggesWp6cKAiYATxlj0oBqwNN7hyHABcAb3f3e/lIICoEBbb7v73pOdQPX/fa3gKXGmLftztMVrtsAnwEzbY7SmZOBC1z33l8FzhSRl+2N1DFjTKHrn4eAd2i5JeupCoCCNleEb9JSGDzZuUC2MeZgd7+xvxSCtcAwERnsqqqXAyttzuQTXA3YxcA2Y8xDdufpjIgkiEiM6+swWgYPbLc1VCeMMXcZY/obY5Jp+TP7b2PMlTbHapeIRLgGC+C6xXI24LGj3owxRcA+ERnheuoswOMGOBxlDhbcFoKWyyOfZ4xpFJGbgY+AQOB5Y8wWm2N1SESWAacD8SJSAPzRGLPY3lQdOhm4CtjkuvcOcLcxZpV9kTrUD3jRNfIiAHjdGOPRQzK9SB/gnZbfCwgCXjHGfGhvpGO6BVjq+uUwF5hrc54OuYrrDOBGS97fH4aPKqWU6pi/3BpSSinVAS0ESinl57QQKKWUn9NCoJRSfk4LgVJK+TktBMpviEjTUas4djqTVETmi8jV3XDefBGJP46fO0dE7hOROBH54ERzKNURv5hHoJTLEdfyEl1ijHnawixdcQots59PoWWlVKUsoVcEyu+5fmP/f6619NeIyFDX838SkV+7vr7VtefCRhF51fVcnIgsdz2XKSLjXc/3EpGPXXsePAdIm3Nd6TpHjog845rcdnSey1yT826lZdG5Z4G5IqKz4ZUltBAofxJ21K2hy9q8Vm6MGQcspOXD92h3AmnGmPHAfNdz9wHrXc/dDbzkev6PwGpjzBha1twZCCAio4DLgJNdVyZNwM+OPpEx5jVaVnHd7Mq0yXXuC47/X12pjumtIeVPOrs1tKzNPx9u5/WNtCxHsBxY7npuOvBTAGPMv11XAlG0rHN/kev590Wk1HX8WcBEYK1rKYYwWpbDbs9wWpY9gJbNdCqP9S+n1PHSQqBUC9PB161m0fIBfz7wexEZdxznEOBFY8xdnR7Uss1jPBAkIluBfq5bRbcYY748jvMq1Sm9NaRUi8va/PObti+ISAAwwBjzGfA7IBroCXyJ69aOiJwOHHbtxfAFcIXr+XOBWNdb/Qu4WER6u16LE5FBRwcxxqQD79Oyi97/A35vjEnVIqCsolcEyp+EtVkhFeBDY0zrENJYEdkI1NGy3G9bgcDLru0NBXjMGFMmIn8Cnnf9XA3wc9fx9wHLRGQL8DWwF8AYs1VE7qFlF68AoAFYALS37eAEWprFNwEevby38n66+qjye9626bpS3U1vDSmllJ/TKwKllPJzekWglFJ+TguBUkr5OS0ESinl57QQKKWUn9NCoJRSfu7/Ax/37gP7k2d0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#episode_rewards = mini_batch_train(env, agent, 50, 500, 64)\n",
    "\n",
    "\n",
    "#TRAINING\n",
    "scores = []                        # list containing scores from each episode\n",
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    \n",
    "for each_iteration in range(num_epochs):\n",
    "    \n",
    "    score = 0\n",
    "    update_step = 0\n",
    "    #soft_target_tau = soft_target_tau * 0.98\n",
    "\n",
    "    env_info=env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "    state = env_info.vector_observations[0]        \n",
    "\n",
    "    for each_environment_step in range(num_steps_per_epoch):\n",
    "    \n",
    "        #sample action from the policy\n",
    "        action = policy.get_action(state)            \n",
    "\n",
    "        #sample transition from the environment\n",
    "        env_info = env.step(action)[brain_name]\n",
    "\n",
    "        next_state = env_info.vector_observations[0]   # get the next state\n",
    "        reward = env_info.rewards[0]                   # get the reward\n",
    "        score += reward\n",
    "        done = env_info.local_done[0]                  # see if episode has finished\n",
    "\n",
    "        #Store the transition in the replay pool\n",
    "        replay_buffer.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        if (replay_buffer.buffer_len() > batch_size) and (each_iteration +1 >episods_before_learning): # and (each_environment_step % update_every == 0):            \n",
    "            update_step+=1\n",
    "            update()\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        if done or each_environment_step == num_steps_per_epoch - 1:\n",
    "            break\n",
    "\n",
    "    \n",
    "    scores_window.append(score)       # save most recent score\n",
    "    scores.append(score)              # save all the scores\n",
    "\n",
    "    #print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(each_iteration, np.mean(scores_window)), end=\"\")\n",
    "    #print (\"Value of Alpha: {:.2f}\".format(alpha.detach().item()),end=\"\\n\")\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    #print('\\rEpisode {}\\tLast Score: {:.2f}; average score: {:.2f}; alpha: {:.4f}'.format(each_iteration, score,np.mean(scores_window),alpha.detach().item()), end=\"\")\n",
    "    print('\\rEpisode {}\\tLast Score: {:.2f}; average score: {:.2f}; alpha: 0'.format(each_iteration, score,np.mean(scores_window)), end=\"\")\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(++len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n",
    "    \n",
    "    if score > 30 and first_30==0:\n",
    "        first_30 = each_iteration\n",
    "\n",
    "    if np.mean(scores_window)>=30.0:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(each_iteration-100, np.mean(scores_window)))\n",
    "        #torch.save(qf1.state_dict(), 'checkpoint_qf1.pth')\n",
    "        #torch.save(policy.state_dict(), 'checkpoint_policy.pth')\n",
    "        break\n",
    "    \n",
    "    \n",
    "env.close()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE DATA INTO CSV FILE FOR FURTHER ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now() \n",
    "experience_name = \"SAC_Fixed_Alpha 0.0035\"\n",
    "date_time = str(now.strftime(\"%d-%m-%Y %H:%M:%S\"))\n",
    "\n",
    "all_scores = np.asarray(scores)\n",
    "\n",
    "solved_ep = each_iteration - 100\n",
    "first_above_30 = first_30\n",
    "\n",
    "total_points = all_scores.sum()\n",
    "variance = np.var(all_scores)\n",
    "scores_per_episod = all_scores.mean()\n",
    "\n",
    "with open('experiences_results.csv', mode='a') as results_file:\n",
    "    results_writer = csv.writer(results_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    results_writer.writerow([experience_name, date_time, solved_ep, first_30, scores_per_episod, variance, all_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
