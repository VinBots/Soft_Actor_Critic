{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control -> SAC with entropy maximization\n",
    "# REACHER\n",
    "\n",
    "---\n",
    "\n",
    "This notebook implements the Soft Actor-Critic Algorithm as documented in the paper [here](https://arxiv.org/pdf/1812.05905.pdf)\n",
    "\n",
    "Useful references\n",
    "[berkeley repository](https://github.com/rail-berkeley/softlearning/blob/master/softlearning/algorithms/sac.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulation parameters\n",
    "\n",
    "train_mode = True #test mode if False\n",
    "load_mode=False #set it to True in test mode (train_mode = False)\n",
    "save_mode = True #save the networks - train_mode only\n",
    "\n",
    "#Task parameters\n",
    "\n",
    "state_dim = 33\n",
    "action_dim = 4\n",
    "solve_score = 30.0\n",
    "\n",
    "# Key Hyperparameters for simulation-purpose\n",
    "\n",
    "auto_entropy_tuning = True\n",
    "init_temp = 1\n",
    "\n",
    "if auto_entropy_tuning==False:\n",
    "    alpha = 0.005\n",
    "\n",
    "single_q = False # this parameter is used to test the influence of the double Q minimization trick\n",
    "\n",
    "\n",
    "# Gemeral Hyperparameters\n",
    "layer_size=128\n",
    "weights_init_bound = 0.999\n",
    "replay_buffer_size=50000\n",
    "num_epochs=1000\n",
    "num_steps_per_epoch=1000\n",
    "batch_size=256\n",
    "discount=0.99\n",
    "soft_target_tau=0.02\n",
    "target_update_period=1\n",
    "\n",
    "policy_lr=0.0005\n",
    "qf_lr=0.0003\n",
    "a_lr = 0.0005\n",
    "\n",
    "update_every = 1\n",
    "episods_before_learning = 10\n",
    "\n",
    "first_30 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import random\n",
    "import math\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n"
     ]
    }
   ],
   "source": [
    "# CPU / GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Launch the environment\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=train_mode)[brain_name]      # reset the environment\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "#print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "#print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "#print('The state for the first agent looks like:', states[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftAgent(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, h1_size, h2_size, output_size):\n",
    "        super(SoftAgent, self).__init__()\n",
    "        \n",
    "        # state, hidden layer, action sizes\n",
    "        self.input_size = input_size\n",
    "        self.h1_size = h1_size\n",
    "        self.h2_size = h2_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # define layers\n",
    "        self.fc1 = nn.Linear(self.input_size, self.h1_size)\n",
    "        self.fc2 = nn.Linear(self.h1_size, self.h2_size)\n",
    "        self.fc3 = nn.Linear(self.h2_size, self.output_size)\n",
    "        \n",
    "        #initialize weights\n",
    "        init_w = 3e-3\n",
    "        self.fc3.weight.data.uniform_(-init_w,init_w)\n",
    "        self.fc3.bias.data.uniform_(-init_w,init_w)            \n",
    "        \n",
    "    def forward(self, state,action):\n",
    "        x = torch.cat([state,action],1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianPolicy(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, h1_size, h2_size, output_mean_size, output_std_size):\n",
    "        super(GaussianPolicy, self).__init__()\n",
    "        \n",
    "        # state, hidden layer, action sizes\n",
    "        self.input_size = input_size\n",
    "        self.h1_size = h1_size\n",
    "        self.h2_size = h2_size\n",
    "        self.output_mean_size = output_mean_size\n",
    "        self.output_std_size = output_std_size\n",
    "        # define layers\n",
    "        self.fc1 = nn.Linear(self.input_size, self.h1_size)\n",
    "        self.fc2 = nn.Linear(self.h1_size, self.h2_size)\n",
    "        self.fc3_mean = nn.Linear(self.h2_size, self.output_mean_size)\n",
    "        self.fc3_log_std = nn.Linear(self.h2_size, self.output_std_size)\n",
    "        #initialize weights\n",
    "        init_w = 3e-3\n",
    "        self.fc3_mean.weight.data.uniform_(-init_w,init_w)\n",
    "        self.fc3_mean.bias.data.uniform_(-init_w,init_w)\n",
    "        self.fc3_log_std.weight.data.uniform_(-init_w,init_w)\n",
    "        self.fc3_log_std.bias.data.uniform_(-init_w,init_w)\n",
    "                        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.fc3_mean(x) #values of the action should be between -1 and 1 so this is not the mean of the action value\n",
    "        log_std = self.fc3_log_std(x)\n",
    "        log_std_min = -20\n",
    "        log_std_max = 0\n",
    "        log_std = torch.clamp(log_std,log_std_min, log_std_max)              \n",
    "        return mean,log_std\n",
    "    \n",
    "    def sample (self,state,epsilon = 1e-6):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal (mean,std)\n",
    "        z = normal.rsample()\n",
    "        action = torch.tanh(z) \n",
    "        log_pi = normal.log_prob(z) - torch.log(1 - action.pow(2) + epsilon)\n",
    "        log_pi = log_pi.sum(1,keepdim=True)\n",
    "        return action, log_pi\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()   \n",
    "        normal = Normal(mean, std)\n",
    "        z = normal.sample() #sample an action from a normal distribution with (mean,std)\n",
    "        action = torch.tanh(z) #squeeze the value between -1 and 1\n",
    "        action = action.cpu().detach().squeeze(0).numpy()\n",
    "        return self.rescale_action(action)\n",
    "    \n",
    "    def get_action2(self, state): #used for testing purpose - we remove the stochasticity of the sampling step\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        mean, log_std = self.forward(state)\n",
    "        action = torch.tanh(mean)\n",
    "        action = action.cpu().detach().squeeze(0).numpy()\n",
    "        return self.rescale_action(action)\n",
    "    \n",
    "    def rescale_action(self, action):\n",
    "        action_range=[-1,1]\n",
    "        return action * (action_range[1] - action_range[0]) / 2.0 +\\\n",
    "            (action_range[1] + action_range[0]) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        #dones = dones.view(dones.size(0), -1)\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def buffer_len(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "qf1 = SoftAgent(\n",
    "    input_size = state_dim + action_dim,\n",
    "    h1_size = layer_size,\n",
    "    h2_size = layer_size,\n",
    "    output_size=1\n",
    ").to(device)\n",
    "qf1_optimizer = optim.Adam(qf1.parameters(), lr=qf_lr)\n",
    "\n",
    "qf2 = SoftAgent(\n",
    "    input_size = state_dim + action_dim,\n",
    "    h1_size = layer_size,\n",
    "    h2_size = layer_size,\n",
    "    output_size=1\n",
    ").to(device)\n",
    "qf2_optimizer = optim.Adam(qf2.parameters(), lr=qf_lr)\n",
    "\n",
    "target_qf1 = SoftAgent(\n",
    "    input_size = state_dim + action_dim,\n",
    "    h1_size = layer_size,\n",
    "    h2_size = layer_size,\n",
    "    output_size=1\n",
    ").to(device)\n",
    "\n",
    "target_qf2 = SoftAgent(\n",
    "    input_size = state_dim + action_dim,\n",
    "    h1_size = layer_size,\n",
    "    h2_size = layer_size,\n",
    "    output_size=1\n",
    ").to(device)\n",
    "\n",
    "policy = GaussianPolicy(\n",
    "    input_size = state_dim,\n",
    "    h1_size = layer_size,\n",
    "    h2_size = layer_size,\n",
    "    output_mean_size = action_dim,\n",
    "    output_std_size = action_dim\n",
    ").to(device)\n",
    "policy_optimizer = optim.Adam(policy.parameters(), lr=policy_lr)\n",
    "\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size, batch_size, 1)\n",
    "\n",
    "if auto_entropy_tuning:    \n",
    "    target_entropy = 0\n",
    "    #log_alpha = torch.tensor([np.log(init_temp)], requires_grad=True, device=device)\n",
    "    log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "    alpha = log_alpha.exp()\n",
    "    log_alpha_optim = optim.Adam([log_alpha], lr=a_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_mode:\n",
    "    qf1.load_state_dict(torch.load('checkpoint_qf1.pth'))\n",
    "    qf1.eval()\n",
    "    \n",
    "    qf2.load_state_dict(torch.load('checkpoint_qf2.pth'))\n",
    "    qf2.eval()\n",
    "    \n",
    "    policy.load_state_dict(torch.load('checkpoint_policy.pth'))\n",
    "    policy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy parameters of qf1 to target_qf1\n",
    "\n",
    "for target_params, params in zip(target_qf1.parameters(), qf1.parameters()):\n",
    "    target_params.data.copy_(params)\n",
    "\n",
    "for target_params, params in zip(target_qf2.parameters(), qf2.parameters()):\n",
    "    target_params.data.copy_(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    \n",
    "    global alpha\n",
    "    global log_alpha\n",
    "    global update_step\n",
    "    delay_step = 1\n",
    "    global soft_target_tau\n",
    "\n",
    "    states, actions,rewards, next_states, dones = replay_buffer.sample() #returns torch tensors\n",
    "\n",
    "    # POLICY EVALUATION STEP\n",
    "    #Update the Q-function parameters\n",
    "\n",
    "    next_actions, next_log_pis = policy.sample(next_states)\n",
    "\n",
    "    next_qf1 = target_qf1.forward(next_states,next_actions)\n",
    "    next_qf2 = target_qf2.forward(next_states,next_actions)\n",
    "\n",
    "    if single_q:\n",
    "        next_q_target = next_qf1 - alpha * next_log_pis\n",
    "    else:\n",
    "        next_q_target = torch.min(next_qf1,next_qf2) - alpha * next_log_pis\n",
    "\n",
    "    expected_q = rewards + (1 - dones) * discount * next_q_target\n",
    "\n",
    "    curr_qf1 = qf1.forward(states,actions)\n",
    "    curr_qf2 = qf2.forward(states,actions)\n",
    "\n",
    "    qf1_loss = F.mse_loss(curr_qf1, expected_q.detach())\n",
    "    qf2_loss = F.mse_loss(curr_qf2, expected_q.detach())\n",
    "\n",
    "    qf1_optimizer.zero_grad()\n",
    "    qf1_loss.backward()\n",
    "    qf1_optimizer.step()\n",
    "\n",
    "    qf2_optimizer.zero_grad()\n",
    "    qf2_loss.backward()\n",
    "    qf2_optimizer.step()\n",
    "\n",
    "    # POLICY IMPROVEMENT STEP\n",
    "    new_actions,log_pi = policy.sample(states)\n",
    "    if update_step % delay_step ==0:\n",
    "        min_q = torch.min(qf1.forward(states, new_actions),\n",
    "                          qf2.forward(states, new_actions))\n",
    "        #alpha = log_alpha.exp()\n",
    "        policy_loss = (alpha * log_pi - min_q).mean()\n",
    "        \n",
    "        #Update policy weights\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        #Update target network weights at every iteration\n",
    "\n",
    "        for target_params, params in zip(target_qf1.parameters(), qf1.parameters()):\n",
    "            target_params.data.copy_(soft_target_tau * params + (1 - soft_target_tau) * target_params)\n",
    "\n",
    "        for target_params, params in zip(target_qf2.parameters(), qf2.parameters()):\n",
    "            target_params.data.copy_(soft_target_tau * params + (1 - soft_target_tau) * target_params)\n",
    "    \n",
    "    #Adjust entropy temperature\n",
    "    if auto_entropy_tuning:    \n",
    "        log_alpha_optim.zero_grad()\n",
    "        alpha_loss = (log_alpha * (-log_pi - target_entropy).detach()).mean()\n",
    "        alpha_loss.backward()\n",
    "        log_alpha_optim.step()\n",
    "        alpha = log_alpha.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 80\tLast Score: 39.02; average score: 11.69; alpha: 0.0056"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if train_mode:\n",
    "\n",
    "    #TRAINING ALGORITHM\n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    #scores = np.zeros(num_agents)\n",
    "    \n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "\n",
    "    for each_iteration in range(num_epochs):\n",
    "\n",
    "        #score = np.zeros(num_agents)\n",
    "        #soft_target_tau*=0.97\n",
    "\n",
    "        score = 0\n",
    "        update_step = 0\n",
    "\n",
    "        env_info=env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "        state = env_info.vector_observations[0]\n",
    "        #state = env_info.vector_observations\n",
    "        \n",
    "        for each_environment_step in range(num_steps_per_epoch):\n",
    "\n",
    "            #sample action from the policy\n",
    "            action = policy.get_action(state)            \n",
    "\n",
    "\n",
    "            env_info = env.step(action)[brain_name]\n",
    "\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            score += reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "\n",
    "            #Store the transition in the replay pool\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "            if (replay_buffer.buffer_len() > batch_size) and (each_iteration +1 >episods_before_learning): # and (each_environment_step % update_every == 0):            \n",
    "                update_step+=1\n",
    "                update()\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done or each_environment_step == num_steps_per_epoch - 1:\n",
    "                break\n",
    "\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save all the scores\n",
    "        \n",
    "        if each_iteration % 10 ==0:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            if auto_entropy_tuning:\n",
    "                print('\\rEpisode {}\\tLast Score: {:.2f}; average score: {:.2f}; alpha: {:.4f}'.format(each_iteration, score,np.mean(scores_window),alpha.detach().item()), end=\"\")\n",
    "            else:\n",
    "                print('\\rEpisode {}\\tLast Score: {:.2f}; average score: {:.2f}; alpha: {:.4f}'.format(each_iteration, score,np.mean(scores_window),alpha), end=\"\")\n",
    "                \n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111)\n",
    "            plt.plot(np.arange(++len(scores)), scores)\n",
    "            plt.ylabel('Score')\n",
    "            plt.xlabel('Episode #')\n",
    "            plt.show()\n",
    "\n",
    "        if score > solve_score and first_30==0:\n",
    "            first_30 = each_iteration\n",
    "\n",
    "        if np.mean(scores_window)>=solve_score:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(each_iteration-100, np.mean(scores_window)))\n",
    "            break         \n",
    "\n",
    "    env.close()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_mode and save_mode:\n",
    "    torch.save(qf1.state_dict(), 'checkpoint_qf1.pth')\n",
    "    torch.save(qf2.state_dict(), 'checkpoint_qf2.pth')\n",
    "    torch.save(policy.state_dict(), 'checkpoint_policy.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE DATA INTO CSV FILE FOR FURTHER ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_mode:\n",
    "    import csv\n",
    "    from datetime import datetime\n",
    "\n",
    "    now = datetime.now() \n",
    "    experience_name = \"SAC FV\"\n",
    "    date_time = str(now.strftime(\"%d-%m-%Y %H:%M:%S\"))\n",
    "\n",
    "    all_scores = np.asarray(scores)\n",
    "\n",
    "    solved_ep = each_iteration - 100\n",
    "    first_above_30 = first_30\n",
    "\n",
    "    total_points = all_scores.sum()\n",
    "    variance = np.var(all_scores)\n",
    "    scores_per_episod = all_scores.mean()\n",
    "\n",
    "    with open('reacher_results.csv', mode='a') as results_file:\n",
    "        results_writer = csv.writer(results_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        results_writer.writerow([experience_name, date_time, solved_ep, first_30, scores_per_episod, variance, all_scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watch the REACHER!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not train_mode:\n",
    "\n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    t_step=0\n",
    "    rewards_history=[]\n",
    "    nb_episodes = 10\n",
    "    \n",
    "    for episodes in range (nb_episodes):\n",
    "\n",
    "        scores = np.zeros(num_agents) # initialize the score (for each agent) \n",
    "        t_step=0\n",
    "        while True:\n",
    "            actions = policy.get_action2(states)                # select an action (for each agent)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            t_step+=1\n",
    "            rewards_history.append(rewards)\n",
    "            if np.all(dones) or t_step>2000:                   # exit loop if episode finished\n",
    "                break\n",
    "        print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "    print ('Total score: {}'.format(np.sum(rewards_history)))\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
