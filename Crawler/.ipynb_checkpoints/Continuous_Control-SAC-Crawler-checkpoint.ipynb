{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control -> SAC with entropy maximization\n",
    "# CRAWLER\n",
    "\n",
    "---\n",
    "\n",
    "This notebook implements the Soft Actor-Critic Algorithm as documented in the paper [here](https://arxiv.org/pdf/1812.05905.pdf)\n",
    "\n",
    "Useful references\n",
    "[berkeley repository](https://github.com/rail-berkeley/softlearning/blob/master/softlearning/algorithms/sac.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulation parameters\n",
    "train_mode = False #test mode if False\n",
    "load_mode=True #set it to True in test mode (train_mode = False)\n",
    "\n",
    "save_mode = False #save the networks - train_mode only\n",
    "\n",
    "\n",
    "#Task parameters\n",
    "\n",
    "state_dim = 129\n",
    "action_dim = 20\n",
    "solve_score = 2000.0\n",
    "\n",
    "# Key Hyperparameters for simulation-purpose\n",
    "\n",
    "auto_entropy_tuning = True\n",
    "init_temp = 0.1\n",
    "if auto_entropy_tuning==False:\n",
    "    alpha = 0.05\n",
    "\n",
    "single_q = False\n",
    "\n",
    "#Hyperparameters\n",
    "\n",
    "layer_size=1024\n",
    "weights_init_bound = 0.999\n",
    "\n",
    "replay_buffer_size=1000000\n",
    "\n",
    "num_epochs=10000\n",
    "\n",
    "num_steps_per_epoch=10000\n",
    "\n",
    "batch_size=layer_size\n",
    "\n",
    "discount=0.99\n",
    "\n",
    "soft_target_tau=0.005\n",
    "\n",
    "target_update_period=1\n",
    "\n",
    "policy_lr=0.0001\n",
    "qf_lr=policy_lr\n",
    "a_lr = policy_lr\n",
    "\n",
    "update_every = 1\n",
    "episods_before_learning = 0\n",
    "\n",
    "first_30 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import random\n",
    "import math\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: CrawlerBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 129\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 20\n",
      "        Vector Action descriptions: , , , , , , , , , , , , , , , , , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 12\n"
     ]
    }
   ],
   "source": [
    "# CPU / GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Launch the environment\n",
    "env = UnityEnvironment(file_name=\"Crawler.app\")\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=train_mode)[brain_name]      # reset the environment\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "#print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "#print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "#print('The state for the first agent looks like:', states[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftAgent(nn.Module):\n",
    "    def __init__(self, input_size, h1_size, h2_size, output_size):\n",
    "        super(SoftAgent, self).__init__()\n",
    "        \n",
    "        # state, hidden layer, action sizes\n",
    "        self.input_size = input_size\n",
    "        self.h1_size = h1_size\n",
    "        self.h2_size = h2_size\n",
    "        self.output_size = output_size \n",
    "        \n",
    "        \n",
    "        # define layers\n",
    "        self.fc1 = nn.Linear(self.input_size, self.h1_size)\n",
    "        self.fc2 = nn.Linear(self.h1_size, self.h2_size)\n",
    "        self.fc3 = nn.Linear(self.h2_size, self.output_size)\n",
    "        \n",
    "        #initialize weights\n",
    "        init_w = 3e-3\n",
    "        self.fc3.weight.data.uniform_(-init_w,init_w)\n",
    "        self.fc3.bias.data.uniform_(-init_w,init_w)            \n",
    "        \n",
    "    def forward(self, state,action):\n",
    "        x = torch.cat([state,action],1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianPolicy(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, h1_size, h2_size, output_mean_size, output_std_size):\n",
    "        super(GaussianPolicy, self).__init__()\n",
    "        \n",
    "        # state, hidden layer, action sizes\n",
    "        self.input_size = input_size\n",
    "        self.h1_size = h1_size\n",
    "        self.h2_size = h2_size\n",
    "        self.output_mean_size = output_mean_size\n",
    "        self.output_std_size = output_std_size\n",
    "\n",
    "        # define layers\n",
    "        self.fc1 = nn.Linear(self.input_size, self.h1_size)\n",
    "        self.fc2 = nn.Linear(self.h1_size, self.h2_size)\n",
    "\n",
    "        self.fc3_mean = nn.Linear(self.h2_size, self.output_mean_size)\n",
    "        self.fc3_log_std = nn.Linear(self.h2_size, self.output_std_size)\n",
    "        \n",
    "        #initialize weights\n",
    "        init_w = 3e-3\n",
    "        self.fc3_mean.weight.data.uniform_(-init_w,init_w)\n",
    "        self.fc3_mean.bias.data.uniform_(-init_w,init_w)\n",
    "        self.fc3_log_std.weight.data.uniform_(-init_w,init_w)\n",
    "        self.fc3_log_std.bias.data.uniform_(-init_w,init_w)\n",
    "                        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.fc3_mean(x) #values of the action should be between -1 and 1 so this is not the mean of the action value\n",
    "        log_std = self.fc3_log_std(x)\n",
    "        \n",
    "        log_std_min = -20\n",
    "        log_std_max = 0\n",
    "        log_std = torch.clamp(log_std,log_std_min, log_std_max)\n",
    "               \n",
    "        return mean,log_std\n",
    "    \n",
    "    def sample (self,state,epsilon = 1e-6):\n",
    "        \n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal (mean,std)\n",
    "        z = normal.rsample()\n",
    "        action = torch.tanh(z)\n",
    "        \n",
    "        log_pi = normal.log_prob(z) - torch.log(1 - action.pow(2) + epsilon)\n",
    "        log_pi = log_pi.sum(1,keepdim=True)\n",
    "        \n",
    "        return action, log_pi\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        normal = Normal(mean, std)\n",
    "        z = normal.sample()\n",
    "        action = torch.tanh(z)\n",
    "        action = action.cpu().detach().squeeze(0).numpy()\n",
    "        return self.rescale_action(action)\n",
    "    \n",
    "    def get_action2(self, state): #used for testing purpose - we remove the stochasticity of the sampling step\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        mean, log_std = self.forward(state)\n",
    "        action = torch.tanh(mean)\n",
    "        action = action.cpu().detach().squeeze(0).numpy()\n",
    "        return self.rescale_action(action)\n",
    "    \n",
    "    def rescale_action(self, action):\n",
    "        action_range=[-1,1]\n",
    "        return action * (action_range[1] - action_range[0]) / 2.0 +\\\n",
    "            (action_range[1] + action_range[0]) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        #dones = dones.view(dones.size(0), -1)\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def buffer_len(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "qf1 = SoftAgent(\n",
    "    input_size = state_dim + action_dim,\n",
    "    h1_size = layer_size,\n",
    "    h2_size = layer_size,\n",
    "    output_size=1\n",
    ").to(device)\n",
    "qf1_optimizer = optim.Adam(qf1.parameters(), lr=qf_lr)\n",
    "\n",
    "qf2 = SoftAgent(\n",
    "    input_size = state_dim + action_dim,\n",
    "    h1_size = layer_size,\n",
    "    h2_size = layer_size,\n",
    "    output_size=1\n",
    ").to(device)\n",
    "qf2_optimizer = optim.Adam(qf2.parameters(), lr=qf_lr)\n",
    "\n",
    "target_qf1 = SoftAgent(\n",
    "    input_size = state_dim + action_dim,\n",
    "    h1_size = layer_size,\n",
    "    h2_size = layer_size,\n",
    "    output_size=1\n",
    ").to(device)\n",
    "\n",
    "target_qf2 = SoftAgent(\n",
    "    input_size = state_dim + action_dim,\n",
    "    h1_size = layer_size,\n",
    "    h2_size = layer_size,\n",
    "    output_size=1\n",
    ").to(device)\n",
    "\n",
    "policy = GaussianPolicy(\n",
    "    input_size = state_dim,\n",
    "    h1_size = layer_size,\n",
    "    h2_size = layer_size,\n",
    "    output_mean_size = action_dim,\n",
    "    output_std_size = action_dim\n",
    ").to(device)\n",
    "policy_optimizer = optim.Adam(policy.parameters(), lr=policy_lr)\n",
    "\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size, batch_size, 1)\n",
    "\n",
    "if auto_entropy_tuning:    \n",
    "    target_entropy = -20 #torch.prod(torch.Tensor(20,1).to(device)).item()\n",
    "    log_alpha = torch.tensor(np.log(init_temp), requires_grad=True, device=device)\n",
    "    log_alpha_optim = optim.Adam([log_alpha], lr=a_lr)\n",
    "    alpha = log_alpha.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_mode:\n",
    "    qf1.load_state_dict(torch.load('checkpoint_qf1.pth'))\n",
    "    qf1.eval()\n",
    "    \n",
    "    qf2.load_state_dict(torch.load('checkpoint_qf2.pth'))\n",
    "    qf2.eval()\n",
    "    \n",
    "    policy.load_state_dict(torch.load('checkpoint_policy.pth'))\n",
    "    policy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy parameters of qf1 to target_qf1\n",
    "\n",
    "for target_params, params in zip(target_qf1.parameters(), qf1.parameters()):\n",
    "    target_params.data.copy_(params)\n",
    "\n",
    "for target_params, params in zip(target_qf2.parameters(), qf2.parameters()):\n",
    "    target_params.data.copy_(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    \n",
    "    global alpha\n",
    "    global log_alpha\n",
    "    global update_step\n",
    "    delay_step = 1\n",
    "    global soft_target_tau\n",
    "    \n",
    "    states, actions,rewards, next_states, dones = replay_buffer.sample() #returns torch tensors\n",
    "\n",
    "    # POLICY ITERATION STEP\n",
    "    #Update the Q-function parameters\n",
    "\n",
    "    next_actions, next_log_pis = policy.sample(next_states)\n",
    "\n",
    "    next_qf1 = target_qf1.forward(next_states,next_actions)\n",
    "    next_qf2 = target_qf2.forward(next_states,next_actions)\n",
    "\n",
    "    #next_qf1_target = next_qf1 - next_log_pis\n",
    "    if single_q:\n",
    "        next_q_target = next_qf1 - alpha * next_log_pis\n",
    "    else:\n",
    "        next_q_target = torch.min(next_qf1,next_qf2) - alpha * next_log_pis\n",
    "\n",
    "    expected_q = rewards + (1 - dones) * discount * next_q_target\n",
    "\n",
    "    curr_qf1 = qf1.forward(states,actions)\n",
    "    curr_qf2 = qf2.forward(states,actions)\n",
    "\n",
    "    qf1_loss = F.mse_loss(curr_qf1, expected_q.detach())\n",
    "    qf2_loss = F.mse_loss(curr_qf2, expected_q.detach())\n",
    "\n",
    "    qf1_optimizer.zero_grad()\n",
    "    qf1_loss.backward()\n",
    "    qf1_optimizer.step()\n",
    "\n",
    "    qf2_optimizer.zero_grad()\n",
    "    qf2_loss.backward()\n",
    "    qf2_optimizer.step()\n",
    "\n",
    "    # POLICY IMPROVEMENT STEP\n",
    "    new_actions,log_pi = policy.sample(states)\n",
    "    if update_step % delay_step ==0:\n",
    "        \n",
    "        #new_q = qf1.forward(states, new_actions)\n",
    "        #policy_loss = (alpha * new_log_pis - new_q).mean()\n",
    "        #policy_loss = (new_log_pis - new_q).mean()\n",
    "        min_q = torch.min(qf1.forward(states, new_actions),\n",
    "                          qf2.forward(states, new_actions))\n",
    "\n",
    "        #policy_loss = (- min_q).mean()\n",
    "        #policy_loss = (alpha * log_pi - min_q).mean()\n",
    "        alpha = log_alpha.exp()\n",
    "        policy_loss = (alpha * log_pi - min_q).mean()\n",
    "\n",
    "        #Update policy weights\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        #Updatetargetnetworkweights at every iteration\n",
    "\n",
    "        for target_params, params in zip(target_qf1.parameters(), qf1.parameters()):\n",
    "            target_params.data.copy_(soft_target_tau * params + (1 - soft_target_tau) * target_params)\n",
    "\n",
    "        for target_params, params in zip(target_qf2.parameters(), qf2.parameters()):\n",
    "            target_params.data.copy_(soft_target_tau * params + (1 - soft_target_tau) * target_params)\n",
    "    \n",
    "    #Adjust entropy temperature\n",
    "    if auto_entropy_tuning:    \n",
    "        alpha = log_alpha.exp()\n",
    "        log_alpha_optim.zero_grad()\n",
    "        alpha_loss = (alpha * (-log_pi - target_entropy).detach()).mean()\n",
    "        alpha_loss.backward()\n",
    "        log_alpha_optim.step()     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 20\tLast Score: 5.35; average score: 2.74; alpha: 0.0822"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6a8c677b82af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meach_iteration\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m>\u001b[0m\u001b[0mepisods_before_learning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# and (each_environment_step % update_every == 0):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mupdate_step\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-1afe114f7b55>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mqf1_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mqf1_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mqf1_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if train_mode:\n",
    "\n",
    "    #TRAINING ALGORITHM\n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    #scores = np.zeros(num_agents)\n",
    "    \n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "\n",
    "    for each_iteration in range(num_epochs):\n",
    "\n",
    "        #score = np.zeros(num_agents)\n",
    "        score = 0\n",
    "        update_step = 0\n",
    "\n",
    "        env_info=env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "        state = env_info.vector_observations[0]\n",
    "        #state = env_info.vector_observations\n",
    "        \n",
    "        for each_environment_step in range(num_steps_per_epoch):\n",
    "\n",
    "            #sample action from the policy\n",
    "            action = policy.get_action(state)            \n",
    "\n",
    "            new_action = np.asarray([0] * 220)\n",
    "            action2 = np.concatenate((action,new_action))\n",
    "            env_info = env.step(action2)[brain_name]\n",
    "\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            #next_state = env_info.vector_observations\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            score += reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "\n",
    "            #Store the transition in the replay pool\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "            if (replay_buffer.buffer_len() > batch_size) and (each_iteration +1 >episods_before_learning): # and (each_environment_step % update_every == 0):            \n",
    "                update_step+=1\n",
    "                update()\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done or each_environment_step == num_steps_per_epoch - 1:\n",
    "                break\n",
    "\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save all the scores\n",
    "        \n",
    "        if each_iteration % 10 ==0:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            if auto_entropy_tuning:\n",
    "                print('\\rEpisode {}\\tLast Score: {:.2f}; average score: {:.2f}; alpha: {:.4f}'.format(each_iteration, score,np.mean(scores_window),alpha.detach().item()), end=\"\")\n",
    "            else:\n",
    "                print('\\rEpisode {}\\tLast Score: {:.2f}; average score: {:.2f}; alpha: {:.4f}'.format(each_iteration, score,np.mean(scores_window),alpha), end=\"\")\n",
    "                \n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111)\n",
    "            plt.plot(np.arange(++len(scores)), scores)\n",
    "            plt.ylabel('Score')\n",
    "            plt.xlabel('Episode #')\n",
    "            plt.show()\n",
    "\n",
    "        if score > solve_score and first_30==0:\n",
    "            first_30 = each_iteration\n",
    "\n",
    "        if np.mean(scores_window)>=solve_score:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(each_iteration-100, np.mean(scores_window)))\n",
    "            break         \n",
    "        \n",
    "\n",
    "    env.close()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_mode and save_mode:\n",
    "    torch.save(qf1.state_dict(), 'checkpoint_qf1.pth')\n",
    "    torch.save(qf2.state_dict(), 'checkpoint_qf2.pth')\n",
    "    torch.save(policy.state_dict(), 'checkpoint_policy.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE DATA INTO CSV FILE FOR FURTHER ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_mode:\n",
    "    import csv\n",
    "    from datetime import datetime\n",
    "\n",
    "    now = datetime.now() \n",
    "    experience_name = \"SAC FV\"\n",
    "    date_time = str(now.strftime(\"%d-%m-%Y %H:%M:%S\"))\n",
    "\n",
    "    all_scores = np.asarray(scores)\n",
    "\n",
    "    solved_ep = each_iteration - 100\n",
    "    first_above_30 = first_30\n",
    "\n",
    "    total_points = all_scores.sum()\n",
    "    variance = np.var(all_scores)\n",
    "    scores_per_episod = all_scores.mean()\n",
    "\n",
    "    with open('crawler_results.csv', mode='a') as results_file:\n",
    "        results_writer = csv.writer(results_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        results_writer.writerow([experience_name, date_time, solved_ep, first_30, scores_per_episod, variance, all_scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watch the Crawler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_mode:\n",
    "\n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    t_step=0\n",
    "    rewards_history=[]\n",
    "    nb_episodes = 10\n",
    "    \n",
    "    for episodes in range (nb_episodes):\n",
    "\n",
    "        scores = np.zeros(num_agents) # initialize the score (for each agent) \n",
    "        t_step=0\n",
    "        while True:\n",
    "            actions = policy.get_action2(states)                # select an action (for each agent)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            t_step+=1\n",
    "            rewards_history.append(rewards)\n",
    "            if np.all(dones) or t_step>2000:                   # exit loop if episode finished\n",
    "                break\n",
    "        print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "    print ('Total score: {}'.format(np.mean(rewards_history)))\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
